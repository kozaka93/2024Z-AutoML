{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import set_config\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, OrdinalEncoder\n",
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "set_config(transform_output=\"pandas\")\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(\"..\", \"transformers\")))\n",
    "from column_drop import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(\".\", \"..\", \"data\")\n",
    "train_dir = os.path.join(data_dir, \"train\")\n",
    "test_dir = os.path.join(data_dir, \"test\")\n",
    "\n",
    "pipeline_dir = os.path.join(\".\", \"..\", \"pipelines\")\n",
    "os.makedirs(pipeline_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will perform preprocessing steps as:\n",
    "* dropping irrelevant columns\n",
    "* imputing missing values\n",
    "* categorical features encoding\n",
    "* scaling \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping columns\n",
    "drop_cols_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"missing_drop\", DropMissing()),\n",
    "        (\"drop_low_variance\", DropLowVarianceCategorical()),\n",
    "        (\"drop_cardinality\", DropHighCardinality()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# pipeline for imputing and binning numerical cols:\n",
    "numeric_pipeline = Pipeline([(\"numeric_imputer\", SimpleImputer(strategy=\"median\"))])\n",
    "\n",
    "categorical_pipeline = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"encoding\",\n",
    "            OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1),\n",
    "        ),  # label encoder because we have a lot of unique categorical values which will result in great dimention increase\n",
    "        (\"categorical_imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transformer = ColumnTransformer(\n",
    "    [\n",
    "        (\n",
    "            \"cat_pipe\",\n",
    "            categorical_pipeline,\n",
    "            make_column_selector(dtype_include=\"object\"),\n",
    "        ),\n",
    "        (\"num_pipe\", numeric_pipeline, make_column_selector(dtype_include=\"number\")),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# final pipeline for X\n",
    "preprocessing_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"drop\", drop_cols_pipeline),\n",
    "        (\"column_transform\", transformer),\n",
    "        (\"scale\", MinMaxScaler()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./../pipelines/preprocessing_pipeline_raw.joblib']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_path = os.path.join(pipeline_dir, \"preprocessing_pipeline_raw.joblib\")\n",
    "joblib.dump(\n",
    "    preprocessing_pipeline,\n",
    "    pipeline_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for dataset 0\n",
      "\tFitted pipeline.\n",
      "\tSaved X_train.\n",
      "\tSaved X_test.\n",
      "\tSaved pipeline.\n",
      "Running for dataset 1\n",
      "\tFitted pipeline.\n",
      "\tSaved X_train.\n",
      "\tSaved X_test.\n",
      "\tSaved pipeline.\n",
      "Running for dataset 2\n",
      "\tFitted pipeline.\n",
      "\tSaved X_train.\n",
      "\tSaved X_test.\n",
      "\tSaved pipeline.\n",
      "Running for dataset 3\n",
      "\tFitted pipeline.\n",
      "\tSaved X_train.\n",
      "\tSaved X_test.\n",
      "\tSaved pipeline.\n",
      "Running for dataset 4\n",
      "\tFitted pipeline.\n",
      "\tSaved X_train.\n",
      "\tSaved X_test.\n",
      "\tSaved pipeline.\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"Running for dataset {i}\")\n",
    "    preprocessing_pipeline = joblib.load(pipeline_path)\n",
    "\n",
    "    X_train = pd.read_csv(os.path.join(train_dir, f\"X{i}_train.csv\"))\n",
    "    X_train_processed = preprocessing_pipeline.fit_transform(X_train)\n",
    "    print(f\"\\tFitted pipeline.\")\n",
    "    X_train_processed.to_csv(\n",
    "        os.path.join(train_dir, f\"X{i}_train_processed.csv\"), index=False\n",
    "    )\n",
    "    print(f\"\\tSaved X_train.\")\n",
    "\n",
    "    X_test = pd.read_csv(os.path.join(test_dir, f\"X{i}_test.csv\"))\n",
    "    X_test_processed = preprocessing_pipeline.transform(X_test)\n",
    "    X_test_processed.to_csv(\n",
    "        os.path.join(test_dir, f\"X{i}_test_processed.csv\"), index=False\n",
    "    )\n",
    "    print(f\"\\tSaved X_test.\")\n",
    "\n",
    "    joblib.dump(\n",
    "        preprocessing_pipeline,\n",
    "        os.path.join(pipeline_dir, f\"preprocessing_pipeline_{i}.joblib\"),\n",
    "    )\n",
    "    print(f\"\\tSaved pipeline.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
