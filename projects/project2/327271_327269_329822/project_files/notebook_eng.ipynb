{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MedAId \n",
    "## A Package for Predicting Patient States Using Classification Tools \n",
    "#### Authors: Zofia Kamińska, Mateusz Deptuch, Karolina Dunal\n",
    "\n",
    "### Tool Specification\n",
    "Our tool is designed to assist doctors in the medical decision-making process. Its primary goal is to analyze tabular patient data, such as age, weight, cholesterol levels, etc., to predict:\n",
    "- Whether a patient has a particular disease (binary classification).\n",
    "- The severity level of the disease (multiclass classification).\n",
    "- The risk of patient mortality (binary classification).\n",
    "\n",
    "### Key Features\n",
    "- Supports both binary and multiclass classification.\n",
    "- Automated data processing: cleaning, exploratory analysis, and feature preparation.\n",
    "- Interpretation of model results using tools like SHAP.\n",
    "- Comparison of various ML models with different metrics (e.g., accuracy, ROC-AUC, sensitivity, specificity).\n",
    "\n",
    "### Target Audience\n",
    "The target audience includes doctors and medical personnel. The tool is designed for users who:\n",
    "- Want to utilize patient data to make better medical decisions.\n",
    "- Do not have advanced knowledge in programming or machine learning.\n",
    "- Need intuitive visualizations and interpretations of model results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of Existing Solutions\n",
    "Below are existing tools with similar functionalities:\n",
    "\n",
    "### 1. Pharm-AutoML\n",
    "- **Description**: A tool focused on analyzing biomedical data using AutoML. It enables the analysis of genomics, pharmacogenomics, and biomarker data.\n",
    "- **Advantages**: Specialization in biomedical fields, integrated biomarker models.\n",
    "- **Limitations**: Limited application to tabular clinical data.\n",
    "\n",
    "### 2. Cardea (MIT)\n",
    "- **Description**: A machine learning platform focused on predicting patient outcomes based on clinical data such as Electronic Health Records (EHR).\n",
    "- **Advantages**: Excellent integration with EHR, use of advanced models.\n",
    "- **Limitations**: Focus on EHR may hinder application to simpler tabular data.\n",
    "\n",
    "### 3. AutoPrognosis\n",
    "- **Description**: AutoPrognosis is an advanced AutoML platform that automatically optimizes health models and processes medical data, offering a wide range of analyses, including classification, regression, and survival analysis. It allows full customization of processes and algorithm selection.\n",
    "- **Advantages**: Offers advanced features and flexibility, supports diverse models, and provides interpretability tools, making it ideal for specialists with advanced needs.\n",
    "- **Limitations**: While it has extensive capabilities, its use is more complex and requires greater technical knowledge, which can sometimes be a challenge in practice.\n",
    "\n",
    "### 4. MLJAR\n",
    "- **Description**: An AutoML tool that supports tabular data across various domains, including medicine.\n",
    "- **Advantages**: Versatility, user-friendly reports, intuitive to use.\n",
    "- **Limitations**: Lack of medical specialization, which may impact interpretability in clinical contexts.\n",
    "\n",
    "### Comparison with Our Tool\n",
    "Our tool stands out due to its simplicity of use, requiring minimal coding, making it ideal for users without advanced technical knowledge. It is also optimized for medical tabular data, making it more suited for biomedical analyses compared to more general tools. Unlike MLJAR, our results are tailored to the needs of doctors. We also differ from Cardea and Pharm-AutoML, which have narrower use cases. Compared to AutoPrognosis, which offers more advanced features and capabilities, our tool is simpler to use and more intuitive, making it easier to implement in practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool Architecture\n",
    "\n",
    "### Folder Structure:\n",
    "- `data/` - Input data\n",
    "- `medaid/` - Source code of the tool\n",
    "- `tests/` - Unit tests\n",
    "\n",
    "## Data Processing Flow:\n",
    "### The MedAId package consists of three main components:\n",
    "\n",
    "1. **Data Processing** (`preprocessing/`): This component handles loading the data, cleaning, encoding categorical variables, and splitting the dataset into training and testing sets. It also handles any missing values and normalizes numerical features. The preprocessing step ensures that the data is ready for model training by transforming it into a suitable format for machine learning algorithms.\n",
    "\n",
    "2. **Modeling** (`training/`): This component focuses on creating classification models, training them, evaluating and comparing their performance, and saving the models to files. It includes model selection (e.g., logistic regression, random forest, support vector machines), hyperparameter tuning, cross-validation, and model evaluation using metrics like accuracy, precision, recall, and ROC-AUC. The best-performing model is selected and saved for future use.\n",
    "\n",
    "3. **Result Interpretation** (`reporting/`): This component generates visualizations of the model results, creates comprehensive reports, performs SHAP (Shapley Additive Explanations) analysis for model interpretability, and compares various metrics to help users understand the model's decision-making process. It includes graphs like ROC curves, confusion matrices, feature importance plots, and detailed model performance summaries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `preprocessing/`\n",
    "The module responsible for the comprehensive data processing pipeline, which includes the following steps: handling numerical formats, removing text columns, imputing missing data, encoding categorical variables, and feature scaling. This class integrates various data processing components into a single pipeline, allowing for simultaneous management of all required stages.\n",
    "\n",
    "#### `preprocessing.py`\n",
    "#### Data Processing Stages:\n",
    "1. **Handling Numerical Formats**  \n",
    "   The function `handle_numeric_format()` in the `NumericCommaHandler` class handles converting numbers formatted with commas (e.g., `1,000`) into the standard numeric format.\n",
    "\n",
    "2. **Removing Text Columns**  \n",
    "   The `ColumnRemover` class is used to identify and remove text columns based on specified thresholds (e.g., when missing data in a column exceeds a predefined limit).\n",
    "\n",
    "3. **Imputation of Missing Data**  \n",
    "   The imputation function relies on various methods, such as linear regression (in the `Imputer` class) and Random Forest. Correlation thresholds are set to use appropriate imputation algorithms depending on the correlation of columns with other variables.\n",
    "\n",
    "4. **Encoding Categorical Variables**  \n",
    "   Categorical variables, including the target column, are encoded by the `Encoder` class using different encoding methods, including `LabelEncoder` and `OneHotEncoder`.\n",
    "\n",
    "5. **Feature Scaling**  \n",
    "   The `scale()` function in the `Scaler` class scales numerical columns in the DataFrame. Depending on the data distribution (normal or skewed), standardization (for normal distribution) or normalization (for skewed distribution) is applied.\n",
    "\n",
    "#### Key Functions of the `Preprocessing` Class:\n",
    "\n",
    "- **`__init__(target_column, path, imputer_lr_correlation_threshold, imputer_rf_correlation_threshold, categorical_threshold, removal_correlation_threshold)`**:  \n",
    "    Initializes the class object by setting parameters such as the target column, correlation thresholds, and other configuration options. It also creates instances of the appropriate processing components like `NumericCommaHandler`, `ColumnRemover`, `Encoder`, `Scaler`, `Imputer`, and `PreprocessingCsv`.\n",
    "    **Parameter Descriptions:**\n",
    "    - `target_column` (str): The name of the target column.\n",
    "    - `path` (str): The directory path where processing details will be saved.\n",
    "    - `imputer_lr_correlation_threshold` (float): The correlation threshold for imputation using linear regression.\n",
    "    - `imputer_rf_correlation_threshold` (float): The correlation threshold for imputation using Random Forest.\n",
    "    - `categorical_threshold` (float): The threshold for considering a column as text, which is not categorical. It is based on the ratio of unique values to all values; if higher than the threshold, the column is treated as text and removed.\n",
    "    - `removal_correlation_threshold` (float): The correlation threshold for removing highly correlated columns (excluding the target column). Only one column from correlated groups is retained.\n",
    "\n",
    "- **`preprocess(dataframe)`**:  \n",
    "    The main processing function that performs all pipeline steps. It takes a DataFrame as input, processes it through each stage, and returns the processed DataFrame. After each stage, it logs processing details such as text column removal, imputation, encoding, and scaling.\n",
    "\n",
    "- **`get_column_info()`**:  \n",
    "    Returns details about the processing for each column, including information on removed columns, imputation methods, encoding, and scaling.\n",
    "\n",
    "- **`save_column_info(text_column_removal_info, imputation_info, encoding_info, scaling_info)`**:  \n",
    "    Saves processing details to a CSV file. This function uses the `PreprocessingCsv` class to store information about removed columns, imputation, encoding, and scaling.\n",
    "\n",
    "- **`get_target_encoding_info()`**:  \n",
    "    Returns information about the encoding method used for the target column.\n",
    "\n",
    "#### Details of Implementation of Individual Components:\n",
    "The following classes and their methods are implemented in separate files.\n",
    "\n",
    "- **`NumericCommaHandler`** - `numeric_format_handler.py`:  \n",
    "  Handles the conversion of numbers formatted with commas (e.g., `1,000`) into a numeric format, ensuring data consistency within the DataFrame.\n",
    "\n",
    "- **`ColumnRemover`** - `column_removal.py`:  \n",
    "  Allows for the removal of text columns whose values are deemed irrelevant, based on various criteria such as the amount of missing data or correlation with the target column.\n",
    "\n",
    "- **`Imputer`** - `imputer.py`:  \n",
    "  Performs imputation of missing data using different methods, such as linear regression, Random Forest, or other algorithms, depending on correlations with other variables.\n",
    "\n",
    "- **`Encoder`** - `encoder.py`:  \n",
    "  Encodes categorical variables, including the target variable, using `LabelEncoder` and `OneHotEncoder`, and ensures that encoding information and mappings are stored.\n",
    "\n",
    "- **`Scaler`** - `scaler.py`:  \n",
    "  Scales numerical variables, deciding between standardization or normalization based on the detected distribution of data within the columns.\n",
    "\n",
    "- **`PreprocessingCsv`** - `preprocessing_info.py`:  \n",
    "  Saves processing details to a CSV file, enabling the tracking of applied methods and parameters throughout the data processing pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `training/`\n",
    "#### `medaid.py`:\n",
    "This module is used for training models and hyperparameter optimization.\n",
    "1. **`__train(...)__`**: This function handles the entire training process and hyperparameter optimization. It trains various classification models, evaluates their performance, and selects the best-performing model based on specified evaluation metrics.\n",
    "2. **`__search.py__`**: Defines the classes for Random Search and Grid Search, which are used during the hyperparameter optimization process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `reporting/`\n",
    "#### `plots.py`:\n",
    "This module is responsible for generating visualizations to support the analysis of model results, saved appropriately in subdirectories within the main `medaid#` folder:\n",
    "\n",
    "1. **`distribution_plots(aid)`**: Creates histograms and bar plots for input variables.\n",
    "2. **`correlation_plot(aid)`**: Generates a correlation matrix and dependency plots between features and the target variable.\n",
    "3. **`make_confusion_matrix(aid)`**: Generates confusion matrices on the test set for each model.\n",
    "4. **`shap_feature_importance_plot(aid)`**: Visualizes feature importance based on SHAP.\n",
    "5. **`generate_supertree_visualizations(medaid, output_dir)`**: Creates interactive visualizations of SuperTree models.\n",
    "6. **`makeplots(aid)`**: Runs all the above functions, generating a complete set of visualizations.\n",
    "\n",
    "#### `mainreporter.py`:\n",
    "The `MainReporter` class generates an HTML report with the results of data and model analysis. The report includes details about the data, preprocessing, feature distributions, correlation matrices, model results, and their in-depth analysis. The generated report is stored in the `reports/` folder inside the `medaid#` folder.\n",
    "\n",
    "1. **`__init__(self, aid, path)`**: The constructor initializes the path to the result folder and the `aid` object containing data and models.\n",
    "2. **`is_nan(value)`**: A helper function to check if a value is NaN.\n",
    "3. **`generate_report()`**: Generates an HTML report, which includes:\n",
    "   - Basic information about the data (number of rows, columns, unique target classes).\n",
    "   - A preview of the data (first few rows of the DataFrame).\n",
    "   - Details of preprocessing from the CSV file.\n",
    "   - Feature distributions on plots.\n",
    "   - Correlation analysis of features with the target and the full correlation matrix.\n",
    "   - Details of the models used and their results (including Accuracy, Precision, Recall, F1).\n",
    "   - Model-specific details (e.g., confusion matrix, feature importance, tree visualizations).\n",
    "   - DecisionTree and RandomForest tree visualizations.\n",
    "\n",
    "#### `predictexplain.py`:\n",
    "The `PredictExplainer` class generates an explanation report for the model's prediction based on the input data, saved in the `medaid#` folder.\n",
    "\n",
    "1. **`__init__(self, medaid, model)`**: Initializes the `PredictExplainer` class, assigning the `medaid` object and model, and loads preprocessing details from a CSV file.\n",
    "2. **`preprocess_input_data(self, input_data)`**: Preprocesses the input data according to the stored preprocessing details, applying one-hot encoding, label encoding, imputation, and scaling based on previous settings.\n",
    "3. **`analyze_prediction(self, prediction, target_column, prediction_proba)`**: Analyzes the predicted value for the target feature, compares it with the distribution in the dataset, and generates a classification report including a feature importance plot (SHAP) for classification tasks.\n",
    "4. **`generate_html_report(self, df, input_data)`**: Using the other functions, generates an HTML report comparing the input data with the dataset, analyzes the predictions, and generates model interpretability plots.\n",
    "5. **`generate_viz(self, input_data)`**: Generates visualizations for input data using SHAP (for most models) or LIME (for tree-based models).\n",
    "6. **`generate_shap_viz(self, input_data)`**: Generates SHAP visualizations, including a force plot for a single prediction and a summary plot for the entire dataset, saving them as files.\n",
    "7. **`generate_lime_viz(self, input_data)`**: Generates LIME visualizations for input data, saving the explanation plot to an HTML file.\n",
    "8. **`predict_target(input_data)`**: Processes the input data, makes a prediction using the model, analyzes the result, and generates SHAP/LIME visualizations to increase interpretability.\n",
    "9. **`classify_and_analyze_features(df, input_data)`**: Classifies features into binary, categorical-text, categorical-numeric, and continuous-numeric types, then provides detailed HTML reports based on their characteristics.\n",
    "10. **`_analyze_binary(df, column, input_value)`**, **`_analyze_categorical_numbers(df, column, input_value)`**, **`_analyze_categorical_strings(df, column, input_value)`**, and **`_analyze_numerical_continuous(df, column, input_value)`**: These functions generate HTML content for different types of features (binary, categorical-numeric, categorical-text, and continuous-numeric), providing detailed information about the input value, its frequency in the dataset, and additional statistical details (such as comparisons with the mean, median, and standard deviation for continuous features).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the `medaid` Class\n",
    "\n",
    "The `medaid` class is the main object of the tool. It allows you to load data, preprocess it, train models, save results, and generate reports.\n",
    "\n",
    "#### Methods:\n",
    "- **`__medaid()__`**: Constructor of the `MedAId` class, initializes the object with the provided parameters.\n",
    "    - **`dataset_path`**: Path to the CSV file with the data.\n",
    "    - **`target_column`**: Name of the column containing the target variable.\n",
    "    - **`models`**: List of models to test (default: `[\"logistic\", \"tree\", \"random_forest\", \"xgboost\", \"lightgbm\"]`).\n",
    "    - **`metric`**: Metric to optimize for (default: `f1`, possible values: `[\"accuracy\", \"f1\", \"recall\", \"precision\"]`).\n",
    "    - **`path`**: Path to save the results.\n",
    "    - **`search`**: Hyperparameter optimization method (default: `random`).\n",
    "    - **`cv`**: Number of cross-validation splits (default: `3`).\n",
    "    - **`n_iter`**: Number of iterations for hyperparameter optimization (default: `20`).\n",
    "    - **`test_size`**: Size of the test set (default: `0.2`).\n",
    "    - **`n_jobs`**: Number of processor cores to use (default: `1`).\n",
    "    - **`param_grids`**: Dictionary containing the parameter grid for each model.\n",
    "    - **`imputer_lr_correlation_threshold`**: Minimum correlation for linear regression imputation.\n",
    "    - **`imputer_rf_correlation_threshold`**: Minimum correlation for Random Forest imputation.\n",
    "    - **`categorical_threshold`**: Threshold for distinguishing text columns from categorical ones (if the ratio of unique values to total values in a column is greater than this threshold, the column is considered text and removed).\n",
    "    - **`removal_correlation_threshold`**: Correlation threshold for removing strongly correlated columns (except the target variable, only one column from a group of strongly correlated ones remains).\n",
    "\n",
    "- **`preprocess()`**: Conducts preprocessing of the data.\n",
    "- **`train()`**: Performs preprocessing and trains models on the training data, saving the best models and their results.\n",
    "- **`save()`**: Saves the models to the file `medaid.pkl` in the `medaid#/` folder.\n",
    "- **`report()`**: Executes the `generate_report()` function from the `MainReporter` class, returning a report in HTML format with the results of data and model analysis, as described in the `reporting/` section.\n",
    "- **`predict_explain(input_data, model)`**: Generates a report explaining the model's prediction based on input data, which is a single row from the DataFrame (excluding the target column). If the model or input data is not provided, the function uses the default values — the first model from the `best_models` list and the first row from the DataFrame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medaid.medaid import MedAId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aid = MedAId(dataset_path='./data/multiclass/Obesity_Classification.csv',\n",
    "             target_column='Label',\n",
    "             metric=\"f1\",\n",
    "             search=\"random\",\n",
    "             path=\"\",\n",
    "             n_iter=10,\n",
    "             cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "logistic progress: 100%|██████████| 10/10 [00:02<00:00,  3.39it/s]\n",
      "tree progress: 100%|██████████| 10/10 [00:00<00:00, 34.87it/s]\n",
      "random_forest progress: 100%|██████████| 10/10 [00:01<00:00,  5.87it/s]\n",
      "xgboost progress: 100%|██████████| 10/10 [00:00<00:00, 20.88it/s]\n",
      "lightgbm progress: 100%|██████████| 10/10 [00:04<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finishing up...\n",
      "\n",
      "==========  Training complete  ==========\n"
     ]
    }
   ],
   "source": [
    "aid.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aid.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aid.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>best_score</th>\n",
       "      <th>f1</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>test_best_score</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>test_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.965657</td>\n",
       "      <td>0.965657</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.973180</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.909091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tree</td>\n",
       "      <td>0.940752</td>\n",
       "      <td>0.940752</td>\n",
       "      <td>0.942529</td>\n",
       "      <td>0.962835</td>\n",
       "      <td>0.942529</td>\n",
       "      <td>0.865014</td>\n",
       "      <td>0.865014</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.872727</td>\n",
       "      <td>0.863636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xgboost</td>\n",
       "      <td>0.928143</td>\n",
       "      <td>0.928143</td>\n",
       "      <td>0.930624</td>\n",
       "      <td>0.951780</td>\n",
       "      <td>0.930624</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>logistic</td>\n",
       "      <td>0.882606</td>\n",
       "      <td>0.882606</td>\n",
       "      <td>0.884647</td>\n",
       "      <td>0.899600</td>\n",
       "      <td>0.884647</td>\n",
       "      <td>0.953047</td>\n",
       "      <td>0.953047</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.961039</td>\n",
       "      <td>0.954545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lightgbm</td>\n",
       "      <td>0.857823</td>\n",
       "      <td>0.857823</td>\n",
       "      <td>0.860016</td>\n",
       "      <td>0.896312</td>\n",
       "      <td>0.860016</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model  best_score        f1  accuracy  precision    recall  \\\n",
       "0  random_forest    0.965657  0.965657  0.965517   0.973180  0.965517   \n",
       "1           tree    0.940752  0.940752  0.942529   0.962835  0.942529   \n",
       "2        xgboost    0.928143  0.928143  0.930624   0.951780  0.930624   \n",
       "3       logistic    0.882606  0.882606  0.884647   0.899600  0.884647   \n",
       "4       lightgbm    0.857823  0.857823  0.860016   0.896312  0.860016   \n",
       "\n",
       "   test_best_score   test_f1  test_accuracy  test_precision  test_recall  \n",
       "0         0.909091  0.909091       0.909091        0.909091     0.909091  \n",
       "1         0.865014  0.865014       0.863636        0.872727     0.863636  \n",
       "2         1.000000  1.000000       1.000000        1.000000     1.000000  \n",
       "3         0.953047  0.953047       0.954545        0.961039     0.954545  \n",
       "4         1.000000  1.000000       1.000000        1.000000     1.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "aid.models_ranking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Normal Weight': np.int64(0), 'Obese': np.int64(1), 'Overweight': np.int64(2), 'Underweight': np.int64(3)}\n",
      "{'Normal Weight': np.int64(0), 'Obese': np.int64(1), 'Overweight': np.int64(2), 'Underweight': np.int64(3)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Underweight',\n",
       " 'Underweight',\n",
       " 'Overweight',\n",
       " 'Underweight',\n",
       " 'Underweight',\n",
       " 'Normal Weight',\n",
       " 'Overweight',\n",
       " 'Normal Weight',\n",
       " 'Underweight',\n",
       " 'Underweight',\n",
       " 'Overweight',\n",
       " 'Underweight',\n",
       " 'Obese',\n",
       " 'Underweight',\n",
       " 'Normal Weight',\n",
       " 'Normal Weight',\n",
       " 'Normal Weight',\n",
       " 'Normal Weight',\n",
       " 'Obese',\n",
       " 'Overweight']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "aid.predict(aid.X_test.iloc[0:20], model_id=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aid.predict_explain(model=aid.best_models[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/mateuszdeptuch/SCHOOL/AUTOML/projekt2/medaid3'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "aid.path"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
